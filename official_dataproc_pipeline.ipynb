{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import constants\n",
    "import extension_tools\n",
    "import os\n",
    "\n",
    "# import datasets\n",
    "# import log_reg\n",
    "# from dataproc import extract_wvs\n",
    "# from dataproc import get_discharge_summaries\n",
    "# from dataproc import concat_and_split\n",
    "# from dataproc import build_vocab\n",
    "# from dataproc import word_embeddings\n",
    "\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume to start that you have prepared the MIMIC-III note files/data splits based on the CAML preprocessing notebook, available here:\n",
    "https://github.com/jamesmullenbach/caml-mimic/blob/master/notebooks/dataproc_mimic_III.ipynb\n",
    "\n",
    "and have the files in the following directory structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intermediate step: write each note to its own file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOINSERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step one: run cTAKES to perform concept annotation on the notefiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOINSERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step two: extract annotated concepts from the XMI files for each note and write to csv, by dev/test/train splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOINSERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step three: build the concept-text alignment matrices for each note:\n",
    "#### This step also constructs a concept vocabulary file based on concepts with >3 occurrences (in TRAINING data).\n",
    "\n",
    "#### NOTE: this takes some time to run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example, for SNOMED only:\n",
    "extension_tools.get_concept_text_alignment(inpt_file='/data/swiegreffe6/NEW_MIMIC/mimic3/train_full.csv', concepts_file='/data/swiegreffe6/NEW_MIMIC/patient_notes/concepts_train_ALL.csv', outpt_file='/data/swiegreffe6/NEW_MIMIC/extracted_concepts/SNOMED/train_patient_concepts_matrix_SNOMED.p', lst_terms=['SNOMEDCT_US'], dir_name='SNOMED')\n",
    "\n",
    "#or as command, \n",
    "    python extension_tools get_concept_text_alignment --input-dir '/data/swiegreffe6/NEW_MIMIC/mimic3/train_full.csv' --concepts-file '/data/swiegreffe6/NEW_MIMIC/patient_notes/concepts_train_ALL.csv' --output-dir='/data/swiegreffe6/NEW_MIMIC/extracted_concepts/SNOMED/train_patient_concepts_matrix_SNOMED.p' --codes-list 'SNOMEDCT_US' --dir-name='SNOMED'\n",
    "\n",
    "#also outputs the concept_children file to '/data/swiegreffe6/NEW_MIMIC/extracted_concepts/dir_name/concept_vocab_%s_children.txt' % dir_name (hardcoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: this step could use some more work, e.g. overlapping concepts, and hyphenated concepts that can't be mapped to any parent codes later\n",
    "#consider some normalization here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step four: build the child-to-parent code mappings based on the ontology resources we want to use:\n",
    "#### NOTE this file only has to be created once (as long as complete ontologies included), because it's a dictionary for later lookup**\n",
    "\n",
    "File directory needed to run this code:\n",
    "\n",
    "    | constants.DATA_DIR\n",
    "        | Multi_Level_CCS_2015\n",
    "            | ccs_multi_dx_tool_2015.csv\n",
    "            | ccs_multi_pr_tool_2015.csv\n",
    "        | SnomedCT_USEditionRF2_PRODUCTION_20180301T183000Z \n",
    "            | Full\n",
    "                | Terminology\n",
    "                    | sct2_Relationship_Full_US1000124_20180301.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('LEN DIRS MAP before SNOMED add:', 19020)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5008688/5008688 [07:22<00:00, 11326.26it/s]\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dirs_map = extension_tools.get_parent_trees()\n",
    "#make sure constants.DATA_DIR points to directory holding subfolders of concept mappings keys.\n",
    "#this method writes 'code_parents.p' to constants.concept_write_dir.\n",
    "\n",
    "#TODOs/NOTE: this file contains/writes a 'lopsided' SNOMED hierarchy, where for each child we only consider one parent.\n",
    "#We also have not included RXCLASS mapping file here (yet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we update the concept vocabulary to include the newly added codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure path_to_vocab is the location of the (>3 occurrences) concept vocab generated from the training set.\n",
    "#This outputs the updated concept vocabulary (plus the higher-level codes) to out_dir/concept_vocab.txt\n",
    "\n",
    "path_to_vocab = os.path.join(concept_write_dir, 'train_meta_concepts.txt')\n",
    "extension_tools.update_vocab(dirs_map, path_to_vocab, out_dir='/data/swiegreffe6/NEW_MIMIC/extracted_concepts/ICD9_RXNORM_SNOMED/ICD9_SNOMED_augmented_concept_vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
