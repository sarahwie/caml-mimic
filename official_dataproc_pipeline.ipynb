{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import constants\n",
    "import extension_tools\n",
    "import os\n",
    "\n",
    "from dataproc import build_vocab\n",
    "\n",
    "# import datasets\n",
    "# import log_reg\n",
    "# from dataproc import extract_wvs\n",
    "# from dataproc import get_discharge_summaries\n",
    "# from dataproc import concat_and_split\n",
    "# from dataproc import word_embeddings\n",
    "\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We assume to start that you have prepared the MIMIC-III note files/data splits based on the CAML preprocessing notebook, available here:\n",
    "https://github.com/jamesmullenbach/caml-mimic/blob/master/notebooks/dataproc_mimic_III.ipynb\n",
    "\n",
    "and have the files in the following directory structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Intermediate step: write each note to its own file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#TOINSERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step one: run cTAKES to perform concept annotation on the notefiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#TOINSERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step two: extract annotated concepts from the XMI files for each note and write to csv, by dev/test/train splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#TOINSERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step three: build the concept-text alignment matrices for each note:\n",
    "#### This step also constructs a concept vocabulary file based on concepts with >3 occurrences (in TRAINING data).\n",
    "\n",
    "#### NOTE: this takes some time to run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#example, for SNOMED only:\n",
    "extension_tools.get_concept_text_alignment(inpt_file='/data/swiegreffe6/NEW_MIMIC/mimic3/train_full.csv', concepts_file='/data/swiegreffe6/NEW_MIMIC/patient_notes/concepts_train_ALL.csv', outpt_file='/data/swiegreffe6/NEW_MIMIC/extracted_concepts/SNOMED/train_patient_concepts_matrix_SNOMED.p', lst_terms=['SNOMEDCT_US'], dir_name='SNOMED')\n",
    "\n",
    "#or as command, \n",
    "    python extension_tools.py get_concept_text_alignment --input-dir '/data/swiegreffe6/NEW_MIMIC/mimic3/train_full.csv' --concepts-dir '/data/swiegreffe6/NEW_MIMIC/patient_notes/concepts_train_ALL.csv' --output-dir='/data/swiegreffe6/NEW_MIMIC/extracted_concepts/SNOMED/train_patient_concepts_matrix_SNOMED.p' --codes-list 'SNOMEDCT_US' --dir-name='SNOMED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#TODO: this step could use some more work, e.g. overlapping concepts, and hyphenated concepts that can't be mapped to any parent codes later\n",
    "#consider some normalization here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step 3.5: build the concept vocabulary based on occurences in the concept matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#TODO: SEPARATE THIS METHOD FROM THE ABOVE**\n",
    "\n",
    "#also outputs the concept_children file to '/data/swiegreffe6/NEW_MIMIC/extracted_concepts/dir_name/concept_vocab_%s_children.txt' % dir_name (hardcoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Step four: build the child-to-parent code mappings based on the ontology resources we want to use:\n",
    "#### NOTE this file only has to be created once (as long as complete ontologies included), because it's a dictionary for later lookup**\n",
    "\n",
    "File directory needed to run this code:\n",
    "\n",
    "    | constants.DATA_DIR\n",
    "        | Multi_Level_CCS_2015\n",
    "            | ccs_multi_dx_tool_2015.csv\n",
    "            | ccs_multi_pr_tool_2015.csv\n",
    "        | SnomedCT_USEditionRF2_PRODUCTION_20180301T183000Z \n",
    "            | Full\n",
    "                | Terminology\n",
    "                    | sct2_Relationship_Full_US1000124_20180301.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/888405 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('LEN DIRS MAP before SNOMED add:', 19020)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 300170/888405 [00:33<01:05, 9019.54it/s]"
     ]
    }
   ],
   "source": [
    "dirs_map = extension_tools.get_parent_trees()\n",
    "#make sure constants.DATA_DIR points to directory holding subfolders of concept mappings keys.\n",
    "#this method writes 'code_parents.p' to constants.concept_write_dir.\n",
    "\n",
    "#TODOs/NOTE: this file contains/writes a 'lopsided' SNOMED hierarchy, where for each child we only consider one parent.\n",
    "#We also have not included RXCLASS mapping file here (yet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Next, we update the concept vocabulary to include the newly added codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Make sure path_to_vocab is the location of the (>3 occurrences) concept vocab generated from the training set.\n",
    "#This outputs the updated concept vocabulary (plus the higher-level codes) to out_dir/concept_vocab.txt\n",
    "\n",
    "path_to_vocab = ''\n",
    "extension_tools.update_vocab(dirs_map, path_to_vocab, out_dir='/data/swiegreffe6/NEW_MIMIC/extracted_concepts/ICD9_RXNORM_SNOMED/ICD9_SNOMED_augmented_concept_vocab.txt')\n",
    "\n",
    "#or as command:\n",
    "    #python extension_tools.py update_vocab --dirs-map '/data/swiegreffe6/NEW_MIMIC/extracted_concepts/ICD9_SNOMED/ICD9_SNOMED_code_parents.p' --vocab-file '/data/swiegreffe6/NEW_MIMIC/extracted_concepts/SNOMED/concept_vocab_SNOMED_children.txt' --output-dir '/data/swiegreffe6/NEW_MIMIC/extracted_concepts/SNOMED/' --load-dirs-map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### TODOS: for the data sparsity experiments, setup a pipeline to recalculate the vocab.txt file based on the reduced train data:\n",
    "\n",
    "e.g. '/data/swiegreffe6/NEW_MIMIC/mimic3/train_full.csv' and '/data/swiegreffe6/NEW_MIMIC/mimic3/vocab.csv' need to be updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MIMIC_3_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-70fd8aab9e21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mMIMIC_3_DIR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'MIMIC_3_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "MIMIC_3_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/data/swiegreffe6/NEW_MIMIC/mimic3/10_percent/train_full.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-172f80506e94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/data/swiegreffe6/NEW_MIMIC/mimic3/10_percent/train_full.csv'\u001b[0m \u001b[0;31m#TODO: UPDATE THIS LINE FOR EACH DIRECTORY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/data/swiegreffe6/NEW_MIMIC/mimic3/10_percent/vocab.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbuild_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/SWiegreffe/caml_extension/dataproc/build_vocab.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(vocab_min, infile, vocab_filename)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mvocab_filename\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfile\u001b[0m \u001b[0mto\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \"\"\"\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m#header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/data/swiegreffe6/NEW_MIMIC/mimic3/10_percent/train_full.csv'"
     ]
    }
   ],
   "source": [
    "vocab_min = 3\n",
    "tr = '/data/swiegreffe6/NEW_MIMIC/mimic3/10_percent/train_full.csv' #TODO: UPDATE THIS LINE FOR EACH DIRECTORY\n",
    "vname = '/data/swiegreffe6/NEW_MIMIC/mimic3/10_percent/vocab.csv'\n",
    "build_vocab.build_vocab(vocab_min, tr, vname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### and to retrain the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### and to rebuild the concept embeddings based off of the new word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### TODO: (re)calculate word-concept embedding matrix (for both the data sparsity and the concepts change experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### TODOs: pretrain the concept embeddings again when add or remove concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
